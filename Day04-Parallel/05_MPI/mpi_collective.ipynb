{
 "metadata": {
  "name": "",
  "signature": "sha256:1cf30a1b56ebdd227ae006b58d08d1f3ffb5acc014be1afd0be221f65496400e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# MPI - Collective Communication"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## MPI_Bcast:\n",
      "\n",
      "C :  `int MPI_Bcast ( void* buffer, int count, MPI_Datatype datatype, int rank, MPI_Comm comm )`\n",
      "\n",
      "Fortran : `MPI_BCAST ( BUFFER, COUNT, DATATYPE, ROOT, COMM, IERR )`  \n",
      "\n",
      "`INTEGER COUNT, DATATYPE, ROOT, COMM, IERR`  \n",
      "`<type> BUFFER`\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### MPI_Bcast example\n",
      "\n",
      "Write a small program that\n",
      "\n",
      "- Has a double array `x` with 4 elements\n",
      "- On all ranks exempt on rank 5 the value of `x` is -1.0\n",
      "- On rank 5 the value of `x` is 23.0 + index i\n",
      "- Send the value of rank 5 to all other processors\n",
      "- Print \"P: %d after prodcasting x is %f \\n\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi_bcast.c\n",
      "#include <mpi.h>\n",
      "#include <stdio.h>\n",
      "int main(int argc, char *argv[]) {\n",
      "   int rank, ierr, i;\n",
      "   double x[4];\n",
      "   \n",
      "   MPI_Init(&argc, &argv);\n",
      "   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
      "   \n",
      "   for (i=0; i<4; i++) x[i] = -1.0;\n",
      "   if(rank==5) {\n",
      "     for (i=0; i<4; i++) x[i]=23.0+i;\n",
      "   }\n",
      "   printf(\"P:%d before broadcast x is %f %f %f %f\\n\",rank,x[0], x[1], x[2], x[3]);\n",
      "   ierr = MPI_Bcast(&x,4,MPI_DOUBLE,5,MPI_COMM_WORLD);\n",
      "   printf(\"P:%d after broadcast x is %f %f %f %f\\n\",rank,x[0], x[1], x[2], x[3]);\n",
      "   MPI_Finalize();\n",
      "}"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing mpi_bcast.c\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpicc mpi_bcast.c -o mpi_bcast.exe\n",
      "!ibrun mpi_bcast.exe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting up job 3797624\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Setting up parallel environment for MVAPICH2+mpispawn.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting parallel tasks...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:16 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:23 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:19 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:18 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:22 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:26 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:31 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:17 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:24 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:25 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:30 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:20 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:21 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:6 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:12 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:7 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:13 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:15 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:3 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:0 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:8 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:28 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:29 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:2 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:27 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:4 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:5 before broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:9 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:5 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:6 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:7 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:9 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:14 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:21 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:25 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:30 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:29 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:26 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:27 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:28 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:1 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:1 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:2 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:3 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:4 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:11 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:8 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:11 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:10 before broadcast x is -1.000000 -1.000000 -1.000000 -1.000000\r\n",
        "P:10 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:0 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:31 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:22 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:23 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:12 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:14 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:13 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:15 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:16 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:17 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:18 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:19 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:24 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n",
        "P:20 after broadcast x is 23.000000 24.000000 25.000000 26.000000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r\n",
        "TACC: Shutdown complete. Exiting.\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## MPI_Reduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "C : `int MPI_Reduce ( void* send_buffer, void* recv_buffer, int count, MPI_Datatype datatype, MPI_Op operation, int rank, MPI_Comm comm )`\n",
      "\n",
      "Fortran : `MPI_REDUCE ( SEND_BUFFER, RECV_BUFFER, COUNT, DATATYPE, OPERATION, RANK, COMM, ERROR )`\n",
      "\n",
      "        INTEGER COUNT, DATATYPE, OPERATION, COMM, ERROR \n",
      "        <datatype> SEND_BUFFER,RECV_BUFFER`\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### MPI_Reduce example\n",
      "\n",
      "Write a small program that\n",
      "\n",
      "- Has a double array `x` with 4 elements\n",
      "- On all ranks the value of `x` is rank+1\n",
      "- On rank 7 compute the product of all `x` values\n",
      "- Print \"P: %d MPI_PROD result is %f \\n\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi_reduce.c\n",
      "#include <stdio.h>\n",
      "#include <mpi.h>\n",
      "int main(int argc, char *argv[]){\n",
      "   int ierr, rank, i;\n",
      "   int root;\n",
      "   long long result[4], x[4];\n",
      "\n",
      "   MPI_Init(&argc, &argv);\n",
      "   MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
      "   root=7;\n",
      "   for (i=0; i<4; i++) x[i]=rank+1;\n",
      "   \n",
      "   ierr = MPI_Reduce(&x,&result,4,MPI_LONG_LONG,MPI_PROD,root,MPI_COMM_WORLD);\n",
      "\n",
      "   if(rank==root) \n",
      "     for (i=0; i<4; i++) printf(\"P:%d MPI_PROD result is %lld\\n\",rank,result[i]);\n",
      "       \n",
      "   MPI_Finalize();\n",
      "}"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting mpi_reduce.c\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpicc mpi_reduce.c -o mpi_reduce.exe\n",
      "!ibrun -n 16 -o 8 mpi_reduce.exe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting up job 3797624\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Setting up parallel environment for MVAPICH2+mpispawn.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting parallel tasks...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:7 MPI_PROD result is 20922789888000\r\n",
        "P:7 MPI_PROD result is 20922789888000\r\n",
        "P:7 MPI_PROD result is 20922789888000\r\n",
        "P:7 MPI_PROD result is 20922789888000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r\n",
        "TACC: Shutdown complete. Exiting.\r\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "math.factorial(16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "20922789888000"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## MPI_Gather\n",
      "\n",
      "C : `int MPI_Gather ( void* send_buffer, int send_count, MPI_datatype \n",
      "                    send_type, void* recv_buffer, int recv_count, \n",
      "                    MPI_Datatype recv_type, int rank, MPI_Comm comm )`\n",
      "                    \n",
      "Fortran : `MPI_GATHER ( SEND_BUFFER, SEND_COUNT, SEND_TYPE, RECV_BUFFER, \n",
      "                RECV_COUNT, RECV_TYPE, RANK, COMM, ERROR )`\n",
      "\n",
      "       `INTEGER SEND_COUNT, SEND_TYPE, RECV_COUNT, RECV_TYPE, RANK, COMM, ERROR`\n",
      "       `<datatype> SEND_BUFFER, RECV_BUFFER`\n",
      "       \n",
      "Each process (root process included) sends the contents of its send buffer to the root process. The root process receives the messages and stores them in rank order. \n",
      "\n",
      "An alternative description is that the n messages sent by the processes in the group are concatenated in rank order, and the resulting message is received by the root as if by a call to MPI_RECV(recvbuf, recvcount * n, recvtype, . . . ).\n",
      "\n",
      "The receive buffer is ignored for all nonroot processes.\n",
      "\n",
      "General, derived datatypes are allowed for both sendtype and recvtype. The type signature of sendcount, sendtype on process i must be equal to the type signature of recvcount, recvtype at the root. This implies that the amount of data sent must be equal to the amount of data received, pairwise between each process and the root. Distinct type maps between sender and receiver are still allowed.\n",
      "\n",
      "All arguments to the function are significant on process root, while on other processes, only arguments sendbuf, sendcount, sendtype, root, comm are significant. The arguments root and comm must have identical values on all processes.\n",
      "\n",
      "The specification of counts and types should not cause any location on the root to be written more than once. Such a call is erroneous.\n",
      "\n",
      "Note that the recvcount argument at the root indicates the number of items it receives from each process, not the total number of items it receives.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### MPI_Gather example\n",
      "\n",
      "Write a small program that\n",
      "\n",
      "- Each process has a a variable that is set to `27.0+rank`\n",
      "- On rank 7 print out all the values from all other ranks\n",
      "- Print \"P: %d x[%d]  is %f \\n\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi_gather.c\n",
      "#include <stdio.h>\n",
      "#include <mpi.h>\n",
      "  \n",
      "int main(int argc, char *argv[])\n",
      "{\n",
      "   int rank,size;\n",
      "   double x[32],mine;\n",
      "   int sndcnt,rcvcnt;\n",
      "   int i;\n",
      "\n",
      "   MPI_Init(&argc, &argv);\n",
      "   MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
      "   MPI_Comm_size(MPI_COMM_WORLD,&size);\n",
      "  \n",
      "   sndcnt=1;\n",
      "   mine=27.0+rank;\n",
      "   if(rank==7) rcvcnt=1;\n",
      "\n",
      "   MPI_Gather(&mine,sndcnt,MPI_DOUBLE,x,rcvcnt,MPI_DOUBLE,7,MPI_COMM_WORLD);\n",
      "\n",
      "   if(rank==7)\n",
      "    for(i=0;i<size;++i) printf(\"PE:%d x[%d] is %f \\n\",rank,i,x[i]); \n",
      "       \n",
      "   MPI_Finalize();\n",
      "}"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting mpi_gather.c\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpicc mpi_gather.c -o mpi_gather.exe\n",
      "!ibrun mpi_gather.exe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting up job 3797624\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Setting up parallel environment for MVAPICH2+mpispawn.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting parallel tasks...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PE:7 x[0] is 27.000000 \r\n",
        "PE:7 x[1] is 28.000000 \r\n",
        "PE:7 x[2] is 29.000000 \r\n",
        "PE:7 x[3] is 30.000000 \r\n",
        "PE:7 x[4] is 31.000000 \r\n",
        "PE:7 x[5] is 32.000000 \r\n",
        "PE:7 x[6] is 33.000000 \r\n",
        "PE:7 x[7] is 34.000000 \r\n",
        "PE:7 x[8] is 35.000000 \r\n",
        "PE:7 x[9] is 36.000000 \r\n",
        "PE:7 x[10] is 37.000000 \r\n",
        "PE:7 x[11] is 38.000000 \r\n",
        "PE:7 x[12] is 39.000000 \r\n",
        "PE:7 x[13] is 40.000000 \r\n",
        "PE:7 x[14] is 41.000000 \r\n",
        "PE:7 x[15] is 42.000000 \r\n",
        "PE:7 x[16] is 43.000000 \r\n",
        "PE:7 x[17] is 44.000000 \r\n",
        "PE:7 x[18] is 45.000000 \r\n",
        "PE:7 x[19] is 46.000000 \r\n",
        "PE:7 x[20] is 47.000000 \r\n",
        "PE:7 x[21] is 48.000000 \r\n",
        "PE:7 x[22] is 49.000000 \r\n",
        "PE:7 x[23] is 50.000000 \r\n",
        "PE:7 x[24] is 51.000000 \r\n",
        "PE:7 x[25] is 52.000000 \r\n",
        "PE:7 x[26] is 53.000000 \r\n",
        "PE:7 x[27] is 54.000000 \r\n",
        "PE:7 x[28] is 55.000000 \r\n",
        "PE:7 x[29] is 56.000000 \r\n",
        "PE:7 x[30] is 57.000000 \r\n",
        "PE:7 x[31] is 58.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r\n",
        "TACC: Shutdown complete. Exiting.\r\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## MPI_Scatter\n",
      "\n",
      "C: `int MPI_Scatter ( void* send_buffer, int send_count, MPI_datatype send_type,\n",
      "                  void* recv_buffer, int recv_count, MPI_Datatype recv_type, \n",
      "                  int rank, MPI_Comm comm )`\n",
      "                  \n",
      "Fortran: `MPI_Scatter ( SEND_BUFFER, SEND_COUNT, SEND_TYPE, RECV_BUFFER, \n",
      "              RECV_COUNT, RECV_TYPE, RANK, COMM, ERROR )`\n",
      "\n",
      "        INTEGER SEND_COUNT, SEND_TYPE, RECV_COUNT, RECV_TYPE, RANK, COMM, ERROR \n",
      "        <datatype> SEND_BUFFER, RECV_BUFFER\n",
      "        \n",
      "MPI_Scatter is the inverse operation to MPI_Gather.\n",
      "\n",
      "An alternative description is that the root sends a message with MPI_Send(sendbuf, sendcount * n, sendtype, ...). This message is split into n equal segments, the ith segment is sent to the ith process in the group, and each process receives this message as above.\n",
      "\n",
      "The send buffer is ignored for all nonroot processes.\n",
      "\n",
      "The type signature associated with sendcount, sendtype at the root must be equal to the type signature associated with recvcount, recvtype at all processes (however, the type maps may be different). This implies that the amount of data sent must be equal to the amount of data received, pairwise between each process and the root. Distinct type maps between sender and receiver are still allowed.\n",
      "\n",
      "All arguments to the function are significant on process root, while on other processes, only arguments recvbuf, recvcount, recvtype, root, comm are significant. The arguments root and comm must have identical values on all processes.\n",
      "\n",
      "The specification of counts and types should not cause any location on the root to be read more than once."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### MPI_Scatter example\n",
      "\n",
      "Write a small program that\n",
      "\n",
      "- Process 3 has x with 8 values\n",
      "- Distribute one value of x to all processes\n",
      "- Print \"P: %d my value is %f \\n\" on all processes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi_scatter.c\n",
      "#include <stdio.h>\n",
      "#include <mpi.h>\n",
      "\n",
      "#define SIZE 32\n",
      "\n",
      "int main(int argc, char *argv[]) {\n",
      "   int rank,size,i;\n",
      "   double x[SIZE],mine;\n",
      "   int sndcnt,rcvcnt;\n",
      "   MPI_Init(&argc, &argv);\n",
      "   MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
      "   MPI_Comm_size(MPI_COMM_WORLD,&size);\n",
      "   rcvcnt=1;\n",
      "   sndcnt=1;\n",
      "   \n",
      "   if(rank==3) {\n",
      "      for(i=0;i<SIZE;++i) x[i]=23.0+i;\n",
      "      sndcnt=1;\n",
      "   }\n",
      "   MPI_Scatter(x,sndcnt,MPI_DOUBLE,&mine,rcvcnt,MPI_DOUBLE,3,MPI_COMM_WORLD);\n",
      "   for(i=0;i<size;++i)  {\n",
      "      if(rank==i) printf(\"P:%d my value is %f \\n\",rank,mine);\n",
      "      fflush(stdout);\n",
      "      MPI_Barrier(MPI_COMM_WORLD);\n",
      "   }\n",
      "   MPI_Finalize();\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting mpi_scatter.c\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpicc mpi_scatter.c -o mpi_scatter.exe\n",
      "!ibrun mpi_scatter.exe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting up job 3797624\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Setting up parallel environment for MVAPICH2+mpispawn.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting parallel tasks...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:0 my value is 23.000000 \r\n",
        "P:1 my value is 24.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:2 my value is 25.000000 \r\n",
        "P:3 my value is 26.000000 \r\n",
        "P:4 my value is 27.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:5 my value is 28.000000 \r\n",
        "P:6 my value is 29.000000 \r\n",
        "P:7 my value is 30.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:8 my value is 31.000000 \r\n",
        "P:9 my value is 32.000000 \r\n",
        "P:10 my value is 33.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:11 my value is 34.000000 \r\n",
        "P:12 my value is 35.000000 \r\n",
        "P:13 my value is 36.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:14 my value is 37.000000 \r\n",
        "P:15 my value is 38.000000 \r\n",
        "P:16 my value is 39.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:17 my value is 40.000000 \r\n",
        "P:18 my value is 41.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:19 my value is 42.000000 \r\n",
        "P:20 my value is 43.000000 \r\n",
        "P:21 my value is 44.000000 \r\n",
        "P:22 my value is 45.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:23 my value is 46.000000 \r\n",
        "P:24 my value is 47.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:25 my value is 48.000000 \r\n",
        "P:26 my value is 49.000000 \r\n",
        "P:27 my value is 50.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P:28 my value is 51.000000 \r\n",
        "P:29 my value is 52.000000 \r\n",
        "P:30 my value is 53.000000 \r\n",
        "P:31 my value is 54.000000 \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \r\n",
        "TACC: Shutdown complete. Exiting.\r\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}