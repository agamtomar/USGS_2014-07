{
 "metadata": {
  "css": [
   ""
  ],
  "name": "",
  "signature": "sha256:e0a88be4ff30c490f465a9c4ce83d14f39e3a46901cf9cdc9d60d89949f34d5b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# MPI\n",
      "\n",
      "- [MPI Standard](http://www.mpi-forum.org)\n",
      "- MPI Implementations:\n",
      "    - [MPICH](http://www.mpich.org)\n",
      "    - [OPENMPI](http://www.open-mpi.org) (used on the Janus supercomputer)\n",
      "    - Vendor implementations, e.g. Intel, Cray"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Requesting 2 nodes on Stampede\n",
      "\n",
      "`srun -A TG-EAR140026 --pty -p development -t 01:00:00 -N2 -n32 /bin/bash -l`\n",
      "\n",
      "### Special variables from the scheduler\n",
      "\n",
      "`$SLURM_NODELIST`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!echo $SLURM_NODELIST"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "c559-[303-304]\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Compiler\n",
      "\n",
      "### Loading the mpi module with the intel compiler\n",
      "\n",
      "`mpicc`\n",
      "\n",
      "`mpif90`\n",
      "\n",
      "`mpicxx`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "module load openmpi/openmpi-1.6.4_intel-13.0.0_torque-4.2.3_ib \n",
      "mpicc -showme"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/curc/tools/x_86_64/rh5/intel/13.0.0/composer_xe_2013.0.079/bin/intel64/icc -I/curc/tools/x_86_64/rh6/openmpi/1.6.4/intel/13.0.0/torque/4.1.4/ib/include -pthread -L/curc/tools/x_86_64/rh6/openmpi/1.6.4/intel/13.0.0/torque/4.1.4/ib/lib -lmpi -ldl -lm -Wl,--export-dynamic -lrt -lnsl -lutil\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Hello world in MPI\n",
      "\n",
      "Output should look like\n",
      "\n",
      "`mpirun -np 4 -bynode ./chello.exe \n",
      "Hello World from process = 2 on processor node1414\n",
      "Hello World from process = 0 on processor node1414\n",
      "Hello World from process = 1 on processor node1271\n",
      "Hello World from process = 3 on processor node1271\n",
      "Number of mpi processes = 4\n",
      "`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### MPI Functions necessary for implementation\n",
      "\n",
      "- [`MPI_Init`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Init.3.php)\n",
      "- [`MPI_Finalize`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Finalize.3.php)\n",
      "- [`MPI_Comm_size`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Comm_size.3.php)\n",
      "- [`MPI_Comm_rank`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Comm_rank.3.php)\n",
      "- [`MPI_Get_processor_name`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Get_processor_name.3.php)\n",
      "- [`MPI_Barrier`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Barrier.3.php)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### The MPI implementation of the Hello World program"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi_hello.c\n",
      "\n",
      "#include <mpi.h>\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "\n",
      "int main (int argc, char *argv[]) \n",
      "{\n",
      "  int ierr, comm_size, comm_rank, length;\n",
      "  char proc_name[MPI_MAX_PROCESSOR_NAME];\n",
      "\n",
      "  ierr = MPI_Init(&argc, &argv);\n",
      "  ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n",
      "  ierr = MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n",
      "  ierr = MPI_Get_processor_name(proc_name, &length);\n",
      "  printf(\"Hello World from process = %d on processor %s\\n\", comm_rank, proc_name);\n",
      "\n",
      "  ierr = MPI_Barrier(MPI_COMM_WORLD);\n",
      "  if (comm_rank == 0) {\n",
      "    printf(\"Number of mpi processes = %d\\n\", comm_size);\n",
      "  }\n",
      "  ierr = MPI_Finalize();\n",
      "} \n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing mpi_hello.c\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Running the Hello World program"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mpicc mpi_hello.c -o mpi_hello.exe\n",
      "ibrun mpi_hello.exe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting up job 3797624\n",
        "TACC: Setting up parallel environment for MVAPICH2+mpispawn.\n",
        "TACC: Starting parallel tasks...\n",
        "Hello World from process = 11 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 13 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 3 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 2 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 5 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 6 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 1 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 8 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 10 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 4 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 7 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 9 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 12 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 0 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 15 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 14 on processor c559-303.stampede.tacc.utexas.edu\n",
        "Hello World from process = 22 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 27 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 31 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 26 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 23 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 16 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 21 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 25 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 30 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 19 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 17 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 20 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 18 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 29 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 28 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Hello World from process = 24 on processor c559-304.stampede.tacc.utexas.edu\n",
        "Number of mpi processes = 32\n",
        " \n",
        "TACC: Shutdown complete. Exiting.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Sending data around in a ring"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Small message size\n",
      "`N=10`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file ring.f90\n",
      "PROGRAM ring\n",
      "\n",
      "  use mpi\n",
      "\n",
      "  INTEGER :: ierr, n=10, count, numprocs, myid, j, ringtag = 10\n",
      "  integer :: status(MPI_STATUS_SIZE)\n",
      "  double precision, allocatable :: x(:)\n",
      "\n",
      "  CALL MPI_Init(ierr)\n",
      "  call MPI_Comm_size(MPI_COMM_WORLD, numprocs, ierr)\n",
      "  call MPI_Comm_rank(MPI_COMM_WORLD, myid, ierr)\n",
      "  allocate(x(n*numprocs))\n",
      "  x(:) = myid\n",
      "  count = 1\n",
      "  DO j = 1, numprocs-1\n",
      "     CALL MPI_send(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid+1,numprocs), ringtag, MPI_COMM_WORLD, ierr)\n",
      "     count = count + n\n",
      "     CALL MPI_recv(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid-1,numprocs), ringtag, MPI_COMM_WORLD, status, ierr)\n",
      "  END DO\n",
      "  write(*,*) 'here is my answer', SUM(x)\n",
      "  CALL MPI_finalize(ierr)\n",
      "\n",
      "END PROGRAM ring"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing ring.f90\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mpif90 ring.f90 -o ring.exe\n",
      "ibrun  ring.exe"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting up job 3797624\n",
        "TACC: Setting up parallel environment for MVAPICH2+mpispawn.\n",
        "TACC: Starting parallel tasks...\n",
        " here is my answer   1050.00000000000     \n",
        " here is my answer   1200.00000000000     \n",
        " here is my answer   3780.00000000000     \n",
        " here is my answer   100.000000000000     \n",
        " here is my answer   4650.00000000000     \n",
        " here is my answer   4960.00000000000     \n",
        " here is my answer   1360.00000000000     \n",
        " here is my answer   30.0000000000000     \n",
        " here is my answer   910.000000000000     \n",
        " here is my answer  0.000000000000000E+000\n",
        " here is my answer   3000.00000000000     \n",
        " here is my answer   2530.00000000000     \n",
        " here is my answer   4060.00000000000     \n",
        " here is my answer   1530.00000000000     \n",
        " here is my answer   3250.00000000000     \n",
        " here is my answer   3510.00000000000     \n",
        " here is my answer   4350.00000000000     \n",
        " here is my answer   1900.00000000000     \n",
        " here is my answer   2760.00000000000     \n",
        " here is my answer   1710.00000000000     \n",
        " here is my answer   2100.00000000000     \n",
        " here is my answer   210.000000000000     \n",
        " here is my answer   2310.00000000000     \n",
        " here is my answer   60.0000000000000     \n",
        " here is my answer   360.000000000000     \n",
        " here is my answer   10.0000000000000     \n",
        " here is my answer   150.000000000000     \n",
        " here is my answer   780.000000000000     \n",
        " here is my answer   660.000000000000     \n",
        " here is my answer   450.000000000000     \n",
        " here is my answer   550.000000000000     \n",
        " here is my answer   280.000000000000     \n",
        " \n",
        "TACC: Shutdown complete. Exiting.\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Large Message Size\n",
      "`N=10000`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file ring.f90\n",
      "PROGRAM ring\n",
      "\n",
      "  use mpi\n",
      "\n",
      "  INTEGER :: ierr, n=10000, count, numprocs, myid, j, ringtag = 10\n",
      "  integer :: status(MPI_STATUS_SIZE)\n",
      "  double precision, allocatable :: x(:)\n",
      "\n",
      "  CALL MPI_Init(ierr)\n",
      "  call MPI_Comm_size(MPI_COMM_WORLD, numprocs, ierr)\n",
      "  call MPI_Comm_rank(MPI_COMM_WORLD, myid, ierr)\n",
      "  allocate(x(n*numprocs))\n",
      "  x(:) = myid\n",
      "  count = 1\n",
      "  DO j = 1, numprocs-1\n",
      "     CALL MPI_send(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid+1,numprocs), ringtag, MPI_COMM_WORLD, ierr)\n",
      "     count = count + n\n",
      "     CALL MPI_recv(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid-1,numprocs), ringtag, MPI_COMM_WORLD, status, ierr)\n",
      "  END DO\n",
      "  write(*,*) 'here is my answer', SUM(x)\n",
      "  CALL MPI_finalize(ierr)\n",
      "\n",
      "END PROGRAM ring"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting ring.f90\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mpif90 ring.f90 -o ring.exe\n",
      "ibrun ring.exe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Process is terminated.\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a typical deadlock. The program hangs and is not working. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Fixing the ring program\n",
      "\n",
      "- Posting a matching receive to a send"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file ring_pair.f90\n",
      "PROGRAM ring\n",
      "\n",
      "  use mpi\n",
      "\n",
      "  implicit none\n",
      "\n",
      "  INTEGER :: ierr, n=10, count, numprocs, myid, j, ringtag = 10\n",
      "  integer :: status(MPI_STATUS_SIZE)\n",
      "  double precision, allocatable :: x(:)\n",
      "\n",
      "  CALL MPI_Init(ierr)\n",
      "  call MPI_Comm_size(MPI_COMM_WORLD, numprocs, ierr)\n",
      "  call MPI_Comm_rank(MPI_COMM_WORLD, myid, ierr)\n",
      "  allocate(x(n*numprocs))\n",
      "  x(:) = myid\n",
      "  count = 1\n",
      "  DO j = 1, numprocs-1\n",
      "     if (MOD(myid, 2) /= 0) then\n",
      "        CALL MPI_recv(x(count+n), n, MPI_DOUBLE_PRECISION, MOD(myid-1,numprocs), ringtag, MPI_COMM_WORLD, status, ierr)\n",
      "        CALL MPI_send(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid+1,numprocs), ringtag, MPI_COMM_WORLD, ierr)\n",
      "        count = count+n\n",
      "     else\n",
      "        CALL MPI_send(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid+1,numprocs), ringtag, MPI_COMM_WORLD, ierr)\n",
      "        count = count + n\n",
      "        CALL MPI_recv(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid-1,numprocs), ringtag, MPI_COMM_WORLD, status, ierr)\n",
      "     end if\n",
      "  END DO\n",
      "  write(*,*) 'here is my answer', SUM(x)\n",
      "  CALL MPI_finalize(ierr)\n",
      "\n",
      "END PROGRAM ring\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing ring_pair.f90\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mpif90 ring_pair.f90 -o ring_pair.exe\n",
      "ibrun ring_pair.exe"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TACC: Starting up job 3797624\n",
        "TACC: Setting up parallel environment for MVAPICH2+mpispawn.\n",
        "TACC: Starting parallel tasks...\n",
        " here is my answer  0.000000000000000E+000\n",
        " here is my answer   10.0000000000000     \n",
        " here is my answer   30.0000000000000     \n",
        " here is my answer   60.0000000000000     \n",
        " here is my answer   100.000000000000     \n",
        " here is my answer   150.000000000000     \n",
        " here is my answer   210.000000000000     \n",
        " here is my answer   280.000000000000     \n",
        " here is my answer   360.000000000000     \n",
        " here is my answer   660.000000000000     \n",
        " here is my answer   450.000000000000     \n",
        " here is my answer   910.000000000000     \n",
        " here is my answer   780.000000000000     \n",
        " here is my answer   1900.00000000000     \n",
        " here is my answer   3250.00000000000     \n",
        " here is my answer   1530.00000000000     \n",
        " here is my answer   4060.00000000000     \n",
        " here is my answer   3780.00000000000     \n",
        " here is my answer   3510.00000000000     \n",
        " here is my answer   550.000000000000     \n",
        " here is my answer   1050.00000000000     \n",
        " here is my answer   1200.00000000000     \n",
        " here is my answer   1710.00000000000     \n",
        " here is my answer   2760.00000000000     \n",
        " here is my answer   3000.00000000000     \n",
        " here is my answer   4960.00000000000     \n",
        " here is my answer   4650.00000000000     \n",
        " here is my answer   2100.00000000000     \n",
        " here is my answer   1360.00000000000     \n",
        " here is my answer   2310.00000000000     \n",
        " here is my answer   2530.00000000000     \n",
        " here is my answer   4350.00000000000     \n",
        " \n",
        "TACC: Shutdown complete. Exiting.\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}