{
 "metadata": {
  "css": [
   ""
  ],
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# MPI\n",
      "\n",
      "- [MPI Standard](http://www.mpi-forum.org)\n",
      "- MPI Implementations:\n",
      "    - [MPICH](http://www.mpich.org)\n",
      "    - [OPENMPI](http://www.open-mpi.org) (used on the Janus supercomputer)\n",
      "    - Vendor implementations, e.g. Intel, Cray"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Requesting 4 nodes on Janus\n",
      "\n",
      "`qsub -I -lnodes=4:ppn=12 -q janus-debug`\n",
      "\n",
      "### Special variables from the scheduler\n",
      "\n",
      "`$PBS_NODEFILE`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!echo $PBS_NODEFILE"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/curc/torque/4.2.3-redhat_6_x86_64/nodes/node1225/aux//2262033.moab.rc.colorado.edu\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat $PBS_NODEFILE"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node1225\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node0808\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node1233\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n",
        "node0435\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Compiler\n",
      "\n",
      "### Loading the mpi module with the intel compiler\n",
      "\n",
      "`mpicc`\n",
      "\n",
      "`mpif90`\n",
      "\n",
      "`mpicxx`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "module load openmpi/openmpi-1.6.4_intel-13.0.0_torque-4.2.3_ib \n",
      "mpicc -showme"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/curc/tools/x_86_64/rh5/intel/13.0.0/composer_xe_2013.0.079/bin/intel64/icc -I/curc/tools/x_86_64/rh6/openmpi/1.6.4/intel/13.0.0/torque/4.1.4/ib/include -pthread -L/curc/tools/x_86_64/rh6/openmpi/1.6.4/intel/13.0.0/torque/4.1.4/ib/lib -lmpi -ldl -lm -Wl,--export-dynamic -lrt -lnsl -lutil\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Running a program on all nodes\n",
      "\n",
      "- serial or parallel program"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "module load openmpi/openmpi-1.6.4_intel-13.0.0_torque-4.2.3_ib \n",
      "mpiexec --help"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "mpiexec (OpenRTE) 1.6.4\n",
        "\n",
        "Usage: mpiexec [OPTION]...  [PROGRAM]...\n",
        "Start the given program using Open RTE\n",
        "\n",
        "   -am <arg0>            Aggregate MCA parameter set file list\n",
        "   --app <arg0>          Provide an appfile; ignore all other command line\n",
        "                         options\n",
        "   -bind-to-board|--bind-to-board \n",
        "                         Whether to bind processes to specific boards\n",
        "                         (meaningless on 1 board/node)\n",
        "   -bind-to-core|--bind-to-core \n",
        "                         Whether to bind processes to specific cores\n",
        "   -bind-to-none|--bind-to-none \n",
        "                         Do not bind processes to cores or sockets\n",
        "                         (default)\n",
        "   -bind-to-socket|--bind-to-socket \n",
        "                         Whether to bind processes to sockets\n",
        "   -byboard|--byboard    Whether to assign processes round-robin by board\n",
        "                         (equivalent to bynode if only 1 board/node)\n",
        "   -bycore|--bycore      Alias for byslot\n",
        "   -bynode|--bynode      Whether to assign processes round-robin by node\n",
        "   -byslot|--byslot      Whether to assign processes round-robin by slot\n",
        "                         (the default)\n",
        "   -bysocket|--bysocket  Whether to assign processes round-robin by socket\n",
        "-c|-np|--np <arg0>       Number of processes to run\n",
        "   -cf|--cartofile <arg0>  \n",
        "                         Provide a cartography file\n",
        "   -cpus-per-proc|--cpus-per-proc <arg0>  \n",
        "                         Number of cpus to use for each process [default=1]\n",
        "   -cpus-per-rank|--cpus-per-rank <arg0>  \n",
        "                         Synonym for cpus-per-proc\n",
        "-d|-debug-devel|--debug-devel \n",
        "                         Enable debugging of OpenRTE\n",
        "   -debug|--debug        Invoke the user-level debugger indicated by the\n",
        "                         orte_base_user_debugger MCA parameter\n",
        "   -debug-daemons|--debug-daemons \n",
        "                         Enable debugging of any OpenRTE daemons used by\n",
        "                         this application\n",
        "   -debug-daemons-file|--debug-daemons-file \n",
        "                         Enable debugging of any OpenRTE daemons used by\n",
        "                         this application, storing output in files\n",
        "   -debugger|--debugger <arg0>  \n",
        "                         Sequence of debuggers to search for when \"--debug\"\n",
        "                         is used\n",
        "   -default-hostfile|--default-hostfile <arg0>  \n",
        "                         Provide a default hostfile\n",
        "   -display-allocation|--display-allocation \n",
        "                         Display the allocation being used by this job\n",
        "   -display-devel-allocation|--display-devel-allocation \n",
        "                         Display a detailed list (mostly intended for\n",
        "                         developers) of the allocation being used by this\n",
        "                         job\n",
        "   -display-devel-map|--display-devel-map \n",
        "                         Display a detailed process map (mostly intended for\n",
        "                         developers) just before launch\n",
        "   -display-map|--display-map \n",
        "                         Display the process map just before launch\n",
        "   -do-not-launch|--do-not-launch \n",
        "                         Perform all necessary operations to prepare to\n",
        "                         launch the application, but do not actually launch\n",
        "                         it\n",
        "   -do-not-resolve|--do-not-resolve \n",
        "                         Do not attempt to resolve interfaces\n",
        "   -gmca|--gmca <arg0> <arg1>  \n",
        "                         Pass global MCA parameters that are applicable to\n",
        "                         all contexts (arg0 is the parameter name; arg1 is\n",
        "                         the parameter value)\n",
        "-h|--help                This help message\n",
        "-H|-host|--host <arg0>   List of hosts to invoke processes on\n",
        "   --hetero              Indicates that multiple app_contexts are being\n",
        "                         provided that are a mix of 32/64 bit binaries\n",
        "   -hostfile|--hostfile <arg0>  \n",
        "                         Provide a hostfile\n",
        "   -launch-agent|--launch-agent <arg0>  \n",
        "                         Command used to start processes on remote nodes\n",
        "                         (default: orted)\n",
        "   -leave-session-attached|--leave-session-attached \n",
        "                         Enable debugging of OpenRTE\n",
        "   -loadbalance|--loadbalance \n",
        "                         Balance total number of procs across all allocated\n",
        "                         nodes\n",
        "   -machinefile|--machinefile <arg0>  \n",
        "                         Provide a hostfile\n",
        "   -mca|--mca <arg0> <arg1>  \n",
        "                         Pass context-specific MCA parameters; they are\n",
        "                         considered global if --gmca is not used and only\n",
        "                         one context is specified (arg0 is the parameter\n",
        "                         name; arg1 is the parameter value)\n",
        "   -n|--n <arg0>         Number of processes to run\n",
        "   -nolocal|--nolocal    Do not run any MPI applications on the local node\n",
        "   -nooversubscribe|--nooversubscribe \n",
        "                         Nodes are not to be oversubscribed, even if the\n",
        "                         system supports such operation\n",
        "   --noprefix            Disable automatic --prefix behavior\n",
        "   -nperboard|--nperboard <arg0>  \n",
        "                         Launch n processes per board on all allocated\n",
        "                         nodes\n",
        "   -npernode|--npernode <arg0>  \n",
        "                         Launch n processes per node on all allocated nodes\n",
        "   -npersocket|--npersocket <arg0>  \n",
        "                         Launch n processes per socket on all allocated\n",
        "                         nodes\n",
        "   -num-boards|--num-boards <arg0>  \n",
        "                         Number of processor boards/node (1-256) [default:\n",
        "                         1]\n",
        "   -num-cores|--num-cores <arg0>  \n",
        "                         Number of cores/socket (1-256) [default: 1]\n",
        "   -num-sockets|--num-sockets <arg0>  \n",
        "                         Number of sockets/board (1-256) [default: 1]\n",
        "   -ompi-server|--ompi-server <arg0>  \n",
        "                         Specify the URI of the Open MPI server, or the name\n",
        "                         of the file (specified as file:filename) that\n",
        "                         contains that info\n",
        "   -output-filename|--output-filename <arg0>  \n",
        "                         Redirect output from application processes into\n",
        "                         filename.rank\n",
        "   -path|--path <arg0>   PATH to be used to look for executables to start\n",
        "                         processes\n",
        "   -pernode|--pernode    Launch one process per available node on the\n",
        "                         specified number of nodes [no -np => use all\n",
        "                         allocated nodes]\n",
        "   --prefix <arg0>       Prefix where Open MPI is installed on remote nodes\n",
        "   --preload-files <arg0>  \n",
        "                         Preload the comma separated list of files to the\n",
        "                         remote machines current working directory before\n",
        "                         starting the remote process.\n",
        "   --preload-files-dest-dir <arg0>  \n",
        "                         The destination directory to use in conjunction\n",
        "                         with --preload-files. By default the absolute and\n",
        "                         relative paths provided by --preload-files are\n",
        "                         used.\n",
        "-q|--quiet               Suppress helpful messages\n",
        "   -report-bindings|--report-bindings \n",
        "                         Whether to report process bindings to stderr\n",
        "   -report-events|--report-events <arg0>  \n",
        "                         Report events to a tool listening at the specified\n",
        "                         URI\n",
        "   -report-pid|--report-pid <arg0>  \n",
        "                         Printout pid on stdout [-], stderr [+], or a file\n",
        "                         [anything else]\n",
        "   -report-uri|--report-uri <arg0>  \n",
        "                         Printout URI on stdout [-], stderr [+], or a file\n",
        "                         [anything else]\n",
        "   -rf|--rankfile <arg0>  \n",
        "                         Provide a rankfile file\n",
        "-s|--preload-binary      Preload the binary on the remote machine before\n",
        "                         starting the remote process.\n",
        "   -server-wait-time|--server-wait-time <arg0>  \n",
        "                         Time in seconds to wait for ompi-server (default:\n",
        "                         10 sec)\n",
        "   -show-progress|--show-progress \n",
        "                         Output a brief periodic report on launch progress\n",
        "   -slot-list|--slot-list <arg0>  \n",
        "                         List of processor IDs to bind MPI processes to\n",
        "                         (e.g., used in conjunction with rank files)\n",
        "   -stdin|--stdin <arg0>  \n",
        "                         Specify procs to receive stdin [rank, all, none]\n",
        "                         (default: 0, indicating rank 0)\n",
        "   -stride|--stride <arg0>  \n",
        "                         When binding multiple cores to a rank, the step\n",
        "                         size to use between cores [default: 1]\n",
        "   -tag-output|--tag-output \n",
        "                         Tag all output with [job,rank]\n",
        "   -timestamp-output|--timestamp-output \n",
        "                         Timestamp all application process output\n",
        "   -tmpdir|--tmpdir <arg0>  \n",
        "                         Set the root for the session directory tree for\n",
        "                         orterun ONLY\n",
        "   -tv|--tv              Deprecated backwards compatibility flag; synonym\n",
        "                         for \"--debug\"\n",
        "   -use-regexp|--use-regexp \n",
        "                         Use regular expressions for launch\n",
        "-v|--verbose             Be verbose\n",
        "-V|--version             Print version and exit\n",
        "   -wait-for-server|--wait-for-server \n",
        "                         If ompi-server is not already running, wait until\n",
        "                         it is detected (default: false)\n",
        "   -wd|--wd <arg0>       Synonym for --wdir\n",
        "   -wdir|--wdir <arg0>   Set the working directory of the started processes\n",
        "-x <arg0>                Export an environment variable, optionally\n",
        "                         specifying a value (e.g., \"-x foo\" exports the\n",
        "                         environment variable foo and takes its value from\n",
        "                         the current environment; \"-x foo=bar\" exports the\n",
        "                         environment variable name foo and sets its value to\n",
        "                         \"bar\" in the started processes)\n",
        "   -xml|--xml            Provide all output in XML format\n",
        "   -xml-file|--xml-file <arg0>  \n",
        "                         Provide all output in XML format to the specified\n",
        "                         file\n",
        "   -xterm|--xterm <arg0>  \n",
        "                         Create a new xterm window and display output from\n",
        "                         the specified ranks there\n",
        "\n",
        "Report bugs to http://www.open-mpi.org/community/help/\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "module load openmpi/openmpi-1.6.4_intel-13.0.0_torque-4.2.3_ib\n",
      "mpiexec --display-map hostname"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " ========================   JOB MAP   ========================\n",
        "\n",
        " Data for node: node1443\tNum procs: 12\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 0\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 1\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 2\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 3\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 4\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 5\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 6\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 7\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 8\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 9\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 10\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 11\n",
        "\n",
        " Data for node: node1204\tNum procs: 12\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 12\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 13\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 14\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 15\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 16\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 17\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 18\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 19\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 20\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 21\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 22\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 23\n",
        "\n",
        " Data for node: node1431\tNum procs: 12\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 24\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 25\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 26\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 27\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 28\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 29\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 30\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 31\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 32\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 33\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 34\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 35\n",
        "\n",
        " Data for node: node0948\tNum procs: 12\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 36\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 37\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 38\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 39\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 40\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 41\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 42\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 43\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 44\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 45\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 46\n",
        " \tProcess OMPI jobid: [52995,1] Process rank: 47\n",
        "\n",
        " =============================================================\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1443\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1204\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node1431\n",
        "node0948\n",
        "node0948\n",
        "node0948\n",
        "node0948\n",
        "node0948\n",
        "node0948\n",
        "node0948\n",
        "node0948\n",
        "node0948\n",
        "node0948\n",
        "node0948\n",
        "node0948\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "module load openmpi/openmpi-1.6.4_intel-13.0.0_torque-4.2.3_ib\n",
      "mpiexec -np 4 --bynode --display-map hostname"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " ========================   JOB MAP   ========================\n",
        "\n",
        " Data for node: node1443\tNum procs: 1\n",
        " \tProcess OMPI jobid: [53041,1] Process rank: 0\n",
        "\n",
        " Data for node: node1204\tNum procs: 1\n",
        " \tProcess OMPI jobid: [53041,1] Process rank: 1\n",
        "\n",
        " Data for node: node1431\tNum procs: 1\n",
        " \tProcess OMPI jobid: [53041,1] Process rank: 2\n",
        "\n",
        " Data for node: node0948\tNum procs: 1\n",
        " \tProcess OMPI jobid: [53041,1] Process rank: 3\n",
        "\n",
        " =============================================================\n",
        "node1443\n",
        "node1204\n",
        "node1431\n",
        "node0948\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Hello world in MPI\n",
      "\n",
      "Output should look like\n",
      "\n",
      "`mpirun -np 4 -bynode ./chello.exe \n",
      "Hello World from process = 2 on processor node1414\n",
      "Hello World from process = 0 on processor node1414\n",
      "Hello World from process = 1 on processor node1271\n",
      "Hello World from process = 3 on processor node1271\n",
      "Number of mpi processes = 4\n",
      "`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### MPI Functions necessary for implementation\n",
      "\n",
      "- [`MPI_Init`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Init.3.php)\n",
      "- [`MPI_Finalize`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Finalize.3.php)\n",
      "- [`MPI_Comm_size`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Comm_size.3.php)\n",
      "- [`MPI_Comm_rank`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Comm_rank.3.php)\n",
      "- [`MPI_Get_processor_name`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Get_processor_name.3.php)\n",
      "- [`MPI_Barrier`](http://www.open-mpi.org/doc/v1.6/man3/MPI_Barrier.3.php)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### The MPI implementation of the Hello World program"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi_hello.c\n",
      "\n",
      "#include <mpi.h>\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "\n",
      "int main (int argc, char *argv[]) \n",
      "{\n",
      "  int ierr, comm_size, comm_rank, length;\n",
      "  char proc_name[MPI_MAX_PROCESSOR_NAME];\n",
      "\n",
      "  ierr = MPI_Init(&argc, &argv);\n",
      "  ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n",
      "  ierr = MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n",
      "  ierr = MPI_Get_processor_name(proc_name, &length);\n",
      "  printf(\"Hello World from process = %d on processor %s\\n\", comm_rank, proc_name);\n",
      "\n",
      "  ierr = MPI_Barrier(MPI_COMM_WORLD);\n",
      "  if (comm_rank == 0) {\n",
      "    printf(\"Number of mpi processes = %d\\n\", comm_size);\n",
      "  }\n",
      "  ierr = MPI_Finalize();\n",
      "} \n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing mpi_hello.c\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Running the Hello World program"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "module load openmpi/openmpi-1.6.4_intel-13.0.0_torque-4.2.3_ib\n",
      "mpicc mpi_hello.c -o mpi_hello.exe\n",
      "mpirun mpi_hello.exe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hello World from process = 0 on processor node1443\n",
        "Hello World from process = 6 on processor node1443\n",
        "Hello World from process = 7 on processor node1443\n",
        "Hello World from process = 10 on processor node1443\n",
        "Hello World from process = 2 on processor node1443\n",
        "Hello World from process = 5 on processor node1443\n",
        "Hello World from process = 40 on processor node0948\n",
        "Hello World from process = 12 on processor node1204\n",
        "Hello World from process = 42 on processor node0948\n",
        "Hello World from process = 14 on processor node1204\n",
        "Hello World from process = 43 on processor node0948\n",
        "Hello World from process = 16 on processor node1204\n",
        "Hello World from process = 46 on processor node0948\n",
        "Hello World from process = 17 on processor node1204\n",
        "Hello World from process = 36 on processor node0948\n",
        "Hello World from process = 21 on processor node1204\n",
        "Hello World from process = 44 on processor node0948\n",
        "Hello World from process = 23 on processor node1204\n",
        "Hello World from process = 37 on processor node0948\n",
        "Hello World from process = 38 on processor node0948\n",
        "Hello World from process = 39 on processor node0948\n",
        "Hello World from process = 41 on processor node0948\n",
        "Hello World from process = 45 on processor node0948\n",
        "Hello World from process = 47 on processor node0948\n",
        "Hello World from process = 13 on processor node1204\n",
        "Hello World from process = 31 on processor node1431\n",
        "Hello World from process = 15 on processor node1204\n",
        "Hello World from process = 33 on processor node1431\n",
        "Hello World from process = 22 on processor node1204\n",
        "Hello World from process = 35 on processor node1431\n",
        "Hello World from process = 24 on processor node1431\n",
        "Hello World from process = 25 on processor node1431\n",
        "Hello World from process = 27 on processor node1431\n",
        "Hello World from process = 29 on processor node1431\n",
        "Hello World from process = 30 on processor node1431\n",
        "Hello World from process = 1 on processor node1443\n",
        "Hello World from process = 3 on processor node1443\n",
        "Hello World from process = 9 on processor node1443\n",
        "Hello World from process = 11 on processor node1443\n",
        "Hello World from process = 4 on processor node1443\n",
        "Hello World from process = 18 on processor node1204\n",
        "Hello World from process = 19 on processor node1204\n",
        "Hello World from process = 26 on processor node1431\n",
        "Hello World from process = 20 on processor node1204\n",
        "Hello World from process = 28 on processor node1431\n",
        "Hello World from process = 32 on processor node1431\n",
        "Hello World from process = 34 on processor node1431\n",
        "Hello World from process = 8 on processor node1443\n",
        "Number of mpi processes = 48\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Sending data around in a ring"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Small message size\n",
      "`N=10`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file ring.f90\n",
      "PROGRAM ring\n",
      "\n",
      "  use mpi\n",
      "\n",
      "  INTEGER :: ierr, n=10, count, numprocs, myid, j, ringtag = 10\n",
      "  integer :: status(MPI_STATUS_SIZE)\n",
      "  double precision, allocatable :: x(:)\n",
      "\n",
      "  CALL MPI_Init(ierr)\n",
      "  call MPI_Comm_size(MPI_COMM_WORLD, numprocs, ierr)\n",
      "  call MPI_Comm_rank(MPI_COMM_WORLD, myid, ierr)\n",
      "  allocate(x(n*numprocs))\n",
      "  x(:) = myid\n",
      "  count = 1\n",
      "  DO j = 1, numprocs-1\n",
      "     CALL MPI_send(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid+1,numprocs), ringtag, MPI_COMM_WORLD, ierr)\n",
      "     count = count + n\n",
      "     CALL MPI_recv(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid-1,numprocs), ringtag, MPI_COMM_WORLD, status, ierr)\n",
      "  END DO\n",
      "  write(*,*) 'here is my answer', SUM(x)\n",
      "  CALL MPI_finalize(ierr)\n",
      "\n",
      "END PROGRAM ring"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing ring.f90\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mpif90 ring.f90 -o ring.exe\n",
      "mpirun -np 4 ring.exe"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "bash: line 1: openmpif90: command not found\n",
        "bash: line 2: openmpirun: command not found\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Large Message Size\n",
      "`N=10000`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file ring.f90\n",
      "PROGRAM ring\n",
      "\n",
      "  use mpi\n",
      "\n",
      "  INTEGER :: ierr, n=10000, count, numprocs, myid, j, ringtag = 10\n",
      "  integer :: status(MPI_STATUS_SIZE)\n",
      "  double precision, allocatable :: x(:)\n",
      "\n",
      "  CALL MPI_Init(ierr)\n",
      "  call MPI_Comm_size(MPI_COMM_WORLD, numprocs, ierr)\n",
      "  call MPI_Comm_rank(MPI_COMM_WORLD, myid, ierr)\n",
      "  allocate(x(n*numprocs))\n",
      "  x(:) = myid\n",
      "  count = 1\n",
      "  DO j = 1, numprocs-1\n",
      "     CALL MPI_send(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid+1,numprocs), ringtag, MPI_COMM_WORLD, ierr)\n",
      "     count = count + n\n",
      "     CALL MPI_recv(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid-1,numprocs), ringtag, MPI_COMM_WORLD, status, ierr)\n",
      "  END DO\n",
      "  write(*,*) 'here is my answer', SUM(x)\n",
      "  CALL MPI_finalize(ierr)\n",
      "\n",
      "END PROGRAM ring"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting ring.f90\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "module load openmpi/openmpi-1.6.4_intel-13.0.0_torque-4.2.3_ib\n",
      "mpif90 ring.f90 -o ring.exe\n",
      "mpirun -np 4 ring.exe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-6-244ef7e70677>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-244ef7e70677>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    module load openmpi/openmpi-1.6.4_intel-13.0.0_torque-4.2.3_ib\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Fixing the ring program\n",
      "\n",
      "- Posting a matching receive to a send"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file ring_pair.f90\n",
      "PROGRAM ring\n",
      "\n",
      "  use mpi\n",
      "\n",
      "  implicit none\n",
      "\n",
      "  INTEGER :: ierr, n=10, count, numprocs, myid, j, ringtag = 10\n",
      "  integer :: status(MPI_STATUS_SIZE)\n",
      "  double precision, allocatable :: x(:)\n",
      "\n",
      "  CALL MPI_Init(ierr)\n",
      "  call MPI_Comm_size(MPI_COMM_WORLD, numprocs, ierr)\n",
      "  call MPI_Comm_rank(MPI_COMM_WORLD, myid, ierr)\n",
      "  allocate(x(n*numprocs))\n",
      "  x(:) = myid\n",
      "  count = 1\n",
      "  DO j = 1, numprocs-1\n",
      "     if (MOD(myid, 2) /= 0) then\n",
      "        CALL MPI_recv(x(count+n), n, MPI_DOUBLE_PRECISION, MOD(myid-1,numprocs), ringtag, MPI_COMM_WORLD, status, ierr)\n",
      "        CALL MPI_send(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid+1,numprocs), ringtag, MPI_COMM_WORLD, ierr)\n",
      "        count = count+n\n",
      "     else\n",
      "        CALL MPI_send(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid+1,numprocs), ringtag, MPI_COMM_WORLD, ierr)\n",
      "        count = count + n\n",
      "        CALL MPI_recv(x(count), n, MPI_DOUBLE_PRECISION, MOD(myid-1,numprocs), ringtag, MPI_COMM_WORLD, status, ierr)\n",
      "     end if\n",
      "  END DO\n",
      "  write(*,*) 'here is my answer', SUM(x)\n",
      "  CALL MPI_finalize(ierr)\n",
      "\n",
      "END PROGRAM ring\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing ring_pair.f90\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "module load openmpi/openmpi-1.6.4_intel-13.0.0_torque-4.2.3_ib\n",
      "mpif90 ring_pair.f90 -o ring_pair.exe\n",
      "mpirun -np 4 ring_pair.exe"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}